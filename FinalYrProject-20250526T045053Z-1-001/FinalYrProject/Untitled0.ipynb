{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') # Mount to root first\n","\n","# Now you can access your folder:\n","import os\n","speechrecognition_path = os.path.join('/content/drive/MyDrive', 'FinalYrProject')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_ipjTWdGp-p","executionInfo":{"status":"ok","timestamp":1748232164819,"user_tz":-330,"elapsed":35975,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}},"outputId":"c03b5808-f742-4069-e116-358d6fb57a4a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2HRAGUHGJv7","executionInfo":{"status":"ok","timestamp":1748232226004,"user_tz":-330,"elapsed":14112,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}},"outputId":"dabce6fa-0e95-4799-ec66-a94804a37abf"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Install required libraries\n","!pip install librosa gradio --quiet"]},{"cell_type":"code","source":["import pickle\n","import librosa\n","import gradio as gr\n","import numpy as np\n","from librosa.feature import zero_crossing_rate, chroma_stft, mfcc, rms, melspectrogram"],"metadata":{"id":"FLeyDE4DKO-L","executionInfo":{"status":"ok","timestamp":1748232262067,"user_tz":-330,"elapsed":31553,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Debugging: Check if librosa has 'feature'\n","print(\"Librosa available functions: \", dir(librosa))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-kuDkrNLPzU","executionInfo":{"status":"ok","timestamp":1748232278991,"user_tz":-330,"elapsed":57,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}},"outputId":"3b22011c-57e0-4725-95a3-25988fbbe216"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Librosa available functions:  ['A4_to_tuning', 'A_weighting', 'B_weighting', 'C_weighting', 'D_weighting', 'LibrosaError', 'ParameterError', 'Z_weighting', 'amplitude_to_db', 'autocorrelate', 'beat', 'blocks_to_frames', 'blocks_to_samples', 'blocks_to_time', 'cache', 'chirp', 'cite', 'clicks', 'core', 'cqt', 'cqt_frequencies', 'db_to_amplitude', 'db_to_power', 'decompose', 'display', 'effects', 'estimate_tuning', 'ex', 'example', 'f0_harmonics', 'feature', 'fft_frequencies', 'fifths_to_note', 'filters', 'fmt', 'fourier_tempo_frequencies', 'frames_to_samples', 'frames_to_time', 'frequency_weighting', 'get_duration', 'get_fftlib', 'get_samplerate', 'griffinlim', 'griffinlim_cqt', 'hybrid_cqt', 'hz_to_fjs', 'hz_to_mel', 'hz_to_midi', 'hz_to_note', 'hz_to_octs', 'hz_to_svara_c', 'hz_to_svara_h', 'icqt', 'iirt', 'interp_harmonics', 'interval_frequencies', 'interval_to_fjs', 'istft', 'key_to_degrees', 'key_to_notes', 'list_mela', 'list_thaat', 'load', 'lpc', 'magphase', 'mel_frequencies', 'mel_to_hz', 'mela_to_degrees', 'mela_to_svara', 'midi_to_hz', 'midi_to_note', 'midi_to_svara_c', 'midi_to_svara_h', 'mu_compress', 'mu_expand', 'multi_frequency_weighting', 'note_to_hz', 'note_to_midi', 'note_to_svara_c', 'note_to_svara_h', 'octs_to_hz', 'onset', 'pcen', 'perceptual_weighting', 'phase_vocoder', 'piptrack', 'pitch_tuning', 'plimit_intervals', 'power_to_db', 'pseudo_cqt', 'pyin', 'pythagorean_intervals', 'reassigned_spectrogram', 'resample', 'salience', 'samples_like', 'samples_to_frames', 'samples_to_time', 'segment', 'sequence', 'set_fftlib', 'show_versions', 'stft', 'stream', 'tempo_frequencies', 'thaat_to_degrees', 'time_to_frames', 'time_to_samples', 'times_like', 'to_mono', 'tone', 'tuning_to_A4', 'util', 'vqt', 'yin', 'zero_crossings']\n"]}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/FinalYrProject/model.pkl'\n","encoder_path = '/content/drive/MyDrive/FinalYrProject/encoder.pkl'\n","scaler_path = '/content/drive/MyDrive/FinalYrProject/scaler.pkl'\n","\n","with open(model_path, 'rb') as model_file:\n","    model = pickle.load(model_file)\n","\n","with open(encoder_path, 'rb') as encoder_file:\n","    encoder = pickle.load(encoder_file)\n","\n","with open(scaler_path, 'rb') as scaler_file:\n","    scaler = pickle.load(scaler_file)"],"metadata":{"id":"PLLgfSzqLaHv","executionInfo":{"status":"ok","timestamp":1748232290650,"user_tz":-330,"elapsed":6099,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","\n","def stretch(data, rate=0.8):\n","    return librosa.effects.time_stretch(y=data, rate=rate)\n","\n","\n","def pitch(data, sampling_rate, pitch_factor=0.7):\n","    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"],"metadata":{"id":"E_q5NnfALdcj","executionInfo":{"status":"ok","timestamp":1748232320799,"user_tz":-330,"elapsed":15,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def extract_features(data, sample_rate):\n","    \"\"\"Extract multiple features from an audio file.\"\"\"\n","    zcr = np.mean(zero_crossing_rate(y=data).T, axis=0)\n","    chroma = np.mean(chroma_stft(S=np.abs(librosa.stft(data)), sr=sample_rate).T, axis=0)\n","    mfcc_feat = np.mean(mfcc(y=data, sr=sample_rate).T, axis=0)\n","    rms_feat = np.mean(rms(y=data).T, axis=0)\n","    mel_feat = np.mean(melspectrogram(y=data, sr=sample_rate).T, axis=0)\n","\n","    return np.hstack([zcr, chroma, mfcc_feat, rms_feat, mel_feat])\n","\n","\n","def get_features(audio):\n","    \"\"\"Process the audio file and extract features with augmentations.\"\"\"\n","    data, sample_rate = librosa.load(audio, sr=None)\n","    result = np.array([extract_features(data, sample_rate)])\n","\n","    # Augmentations\n","    result = np.vstack([result, extract_features(noise(data), sample_rate)])\n","    result = np.vstack([result, extract_features(pitch(stretch(data), sample_rate), sample_rate)])\n","\n","    return result"],"metadata":{"id":"eOEkE1q9Mge0","executionInfo":{"status":"ok","timestamp":1748232332464,"user_tz":-330,"elapsed":20,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def predict_emotion(audio):\n","    \"\"\"Predict the emotion from an audio file.\"\"\"\n","    features = get_features(audio)\n","    prediction = model.predict(features)\n","    return encoder.inverse_transform(prediction)[0]\n","\n","# Create Gradio Interface\n","gr.Interface(\n","    fn=predict_emotion,\n","    inputs=gr.Audio(type=\"filepath\"),\n","    outputs=gr.Textbox(label=\"Predicted Emotion\"),\n","    title=\"Speech Emotion Recognition\",\n","    description=\"Upload or record your voice to predict the emotion.\"\n",").launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"HhYvE2k8MnVm","executionInfo":{"status":"ok","timestamp":1748232340942,"user_tz":-330,"elapsed":2277,"user":{"displayName":"Alisha Raghav","userId":"02208612860651339101"}},"outputId":"d23ecaea-775f-4b97-fa09-79f6e40db85a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://e4f4eab76cc37536df.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://e4f4eab76cc37536df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":8}]}]}